/**
 * Vercel Serverless Function â€” Chat com GPT-5.2
 * Proxy SSE streaming para a API OpenAI.
 *
 * O frontend envia: { messages, context }
 * A funÃ§Ã£o monta um system prompt DINÃ‚MICO com os dados reais do usuÃ¡rio
 * e retransmite o streaming SSE.
 */

const OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions';
const MODEL = 'gpt-5.2';

// â”€â”€â”€ System Prompt Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const AXIOM_BASE_PROMPT = `VocÃª Ã© **Axiom**, um estrategista de vida conversacional integrado ao App Axiom.

## Suas capacidades:
- ğŸ’° **FinanÃ§as**: Analisar transaÃ§Ãµes, contas, parcelamentos, assinaturas reais do usuÃ¡rio.
- ğŸ¯ **HÃ¡bitos**: Acompanhar streaks, sugerir novos hÃ¡bitos, motivar consistÃªncia.
- âœ… **Tarefas**: Organizar tarefas, sugerir prioridades, acompanhar deadlines.
- ğŸ“ **Projetos**: Acompanhar progresso de projetos e sub-tarefas.
- ğŸ“ **MemÃ³ria**: Usar notas, diÃ¡rio e memÃ³rias salvas como contexto.
- ğŸ“Š **AnÃ¡lises**: Gerar insights do Axiom Score (nota geral de organizaÃ§Ã£o de vida).

## Regras:
- Responda SEMPRE em portuguÃªs do Brasil.
- Seja direto, empÃ¡tico e acionÃ¡vel.
- Use emojis com moderaÃ§Ã£o (1-2 por mensagem mÃ¡ximo).
- Respostas devem ser curtas (2-4 parÃ¡grafos mÃ¡ximo).
- Quando o usuÃ¡rio pedir para criar algo (tarefa, transaÃ§Ã£o, hÃ¡bito), sugira a aÃ§Ã£o e confirme com ele.
- VocÃª TEM ACESSO aos dados reais do usuÃ¡rio â€” use-os para dar respostas precisas.
- Nunca invente dados que nÃ£o foram fornecidos no contexto abaixo.
- Se algum dado nÃ£o estiver disponÃ­vel no contexto, informe que aquele dado especÃ­fico nÃ£o foi encontrado.`;

// â”€â”€â”€ Modos de Personalidade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const PERSONALITY_MODES: Record<string, string> = {
    direto: `\n\n## Tom de voz â€” MODO DIRETO ğŸ¯
Seja brutalmente honesto, sem rodeios. VÃ¡ direto ao ponto. NÃ£o enrole, nÃ£o suavize, diga a verdade doa a quem doer. Tipo: "VocÃª gastou demais este mÃªs. Simples assim."`,
    sabio: `\n\n## Tom de voz â€” MODO SÃBIO ğŸ§˜
Seja reflexivo e filosÃ³fico. Guie com perguntas poderosas. Ajude o usuÃ¡rio a encontrar as respostas dentro de si. Tipo: "O que seus gastos dizem sobre suas prioridades?"`,
    parceiro: `\n\n## Tom de voz â€” MODO PARCEIRO ğŸ¤
Seja empÃ¡tico, caloroso e prÃ¡tico. DÃª apoio genuÃ­no e sugira prÃ³ximos passos concretos. Celebre vitÃ³rias, por menores que sejam. Tipo: "Que bom que vocÃª registrou! Vamos ver juntos como melhorar?"`
};

// â”€â”€â”€ Helper: formata moeda BR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function formatBRL(value: number): string {
    return new Intl.NumberFormat('pt-BR', { style: 'currency', currency: 'BRL' }).format(value);
}

// â”€â”€â”€ Monta o system prompt dinÃ¢mico com dados reais â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function buildSystemPrompt(context: any): string {
    let prompt = AXIOM_BASE_PROMPT;

    if (!context) return prompt + PERSONALITY_MODES['direto'];

    // 1. Contexto pessoal (definido nas ConfiguraÃ§Ãµes)
    const profile = context.profile;
    if (profile) {
        prompt += `\n\n## Sobre o UsuÃ¡rio`;
        if (profile.full_name) prompt += `\n- **Nome**: ${profile.full_name}`;
        if (profile.user_context) {
            prompt += `\n- **Contexto pessoal** (definido pelo prÃ³prio usuÃ¡rio nas configuraÃ§Ãµes):\n${profile.user_context}`;
        }
        // Personalidade
        const mode = profile.personality_mode || 'direto';
        prompt += PERSONALITY_MODES[mode] || PERSONALITY_MODES['direto'];
    } else {
        prompt += PERSONALITY_MODES['direto'];
    }

    // 2. MemÃ³rias salvas (coisas que a IA "aprendeu" sobre o usuÃ¡rio)
    if (context.memories && context.memories.length > 0) {
        prompt += `\n\n## ğŸ§  MemÃ³rias Salvas (coisas que vocÃª sabe sobre o usuÃ¡rio)`;
        const grouped: Record<string, string[]> = {};
        context.memories.forEach((m: any) => {
            if (!grouped[m.type]) grouped[m.type] = [];
            grouped[m.type].push(m.content);
        });
        const typeLabels: Record<string, string> = {
            personality: 'Personalidade',
            routine: 'Rotina',
            goal: 'Objetivos',
            pattern: 'PadrÃµes',
            preference: 'PreferÃªncias',
            fact: 'Fatos',
            insight: 'Insights'
        };
        for (const [type, items] of Object.entries(grouped)) {
            prompt += `\n### ${typeLabels[type] || type}`;
            items.forEach(item => { prompt += `\n- ${item}`; });
        }
    }

    // 3. Resumo Financeiro
    if (context.finance) {
        const f = context.finance;
        prompt += `\n\n## ğŸ’° FinanÃ§as â€” ${f.monthLabel}`;
        prompt += `\n- Receita total: ${formatBRL(f.totalIncome)}`;
        prompt += `\n- Despesa total: ${formatBRL(f.totalExpenses)}`;
        prompt += `\n- Saldo do mÃªs: ${formatBRL(f.balance)}`;

        if (f.accounts && f.accounts.length > 0) {
            prompt += `\n### Contas:`;
            f.accounts.forEach((a: any) => {
                prompt += `\n- ${a.name}: ${formatBRL(a.balance)}`;
            });
        }

        if (f.recentTransactions && f.recentTransactions.length > 0) {
            prompt += `\n### Ãšltimas transaÃ§Ãµes:`;
            f.recentTransactions.forEach((t: any) => {
                const icon = t.type === 'income' ? 'ğŸ“ˆ' : 'ğŸ“‰';
                const status = t.paid ? 'âœ…' : 'â³';
                prompt += `\n- ${icon} ${t.title}: ${formatBRL(t.amount)} (${t.category}) ${status} â€” ${t.date}`;
            });
        }
    }

    // 4. HÃ¡bitos
    if (context.habits && context.habits.length > 0) {
        prompt += `\n\n## ğŸ¯ HÃ¡bitos Ativos`;
        context.habits.forEach((h: any) => {
            prompt += `\n- ${h.title}: streak ${h.streak} dias (recorde: ${h.bestStreak}) â€” ${h.frequency}`;
        });
    }

    // 5. Tarefas pendentes
    if (context.tasks && context.tasks.length > 0) {
        prompt += `\n\n## âœ… Tarefas Pendentes`;
        context.tasks.forEach((t: any) => {
            const priorityIcon = t.priority === 'high' ? 'ğŸ”´' : t.priority === 'medium' ? 'ğŸŸ¡' : 'ğŸŸ¢';
            prompt += `\n- ${priorityIcon} ${t.title} (${t.status})${t.dueDate ? ` â€” prazo: ${t.dueDate}` : ''}`;
        });
    }

    // 6. Projetos
    if (context.projects && context.projects.length > 0) {
        prompt += `\n\n## ğŸ“ Projetos Ativos`;
        context.projects.forEach((p: any) => {
            prompt += `\n- ${p.title}: ${p.progress}% (${p.status})${p.dueDate ? ` â€” prazo: ${p.dueDate}` : ''}`;
        });
    }

    // 7. Axiom Score
    if (context.score) {
        const s = context.score;
        prompt += `\n\n## ğŸ“Š Axiom Score (Ãºltimo cÃ¡lculo: ${s.calculated_at?.split('T')[0] || 'N/A'})`;
        prompt += `\n- **Score Total**: ${s.total_score}/100`;
        prompt += `\n- Financeiro: ${s.financial_score} | HÃ¡bitos: ${s.habits_score} | ExecuÃ§Ã£o: ${s.execution_score} | Clareza: ${s.clarity_score} | Projetos: ${s.projects_score}`;
    }

    // 8. Notas recentes
    if (context.notes && context.notes.length > 0) {
        prompt += `\n\n## ğŸ“ Notas Recentes`;
        context.notes.forEach((n: any) => {
            prompt += `\n- **${n.title || 'Sem tÃ­tulo'}**: ${n.content || '(vazia)'}`;
        });
    }

    return prompt;
}

// â”€â”€â”€ Handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const config = {
    runtime: 'edge',
};

export default async function handler(request: Request): Promise<Response> {
    // Apenas POST
    if (request.method !== 'POST') {
        return new Response(JSON.stringify({ error: 'Method not allowed' }), {
            status: 405,
            headers: { 'Content-Type': 'application/json' },
        });
    }

    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
        return new Response(
            JSON.stringify({ error: 'OPENAI_API_KEY nÃ£o configurada no servidor' }),
            { status: 500, headers: { 'Content-Type': 'application/json' } }
        );
    }

    try {
        const body = await request.json();
        const userMessages = body.messages || [];
        const context = body.context || null;

        // Montar system prompt DINÃ‚MICO com dados reais
        const systemPrompt = buildSystemPrompt(context);

        // Montar mensagens com system prompt
        const messages = [
            { role: 'system', content: systemPrompt },
            ...userMessages,
        ];

        const streamMode = body.stream !== false; // default: stream

        // Chamar OpenAI
        const openaiResponse = await fetch(OPENAI_API_URL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                Authorization: `Bearer ${apiKey}`,
            },
            body: JSON.stringify({
                model: MODEL,
                messages,
                stream: streamMode,
                temperature: 0.7,
                max_completion_tokens: 2048,
            }),
        });

        if (!openaiResponse.ok) {
            const errorText = await openaiResponse.text();
            console.error('OpenAI API error:', openaiResponse.status, errorText);
            return new Response(
                JSON.stringify({
                    error: `Erro na API OpenAI: ${openaiResponse.status}`,
                    details: errorText,
                }),
                { status: openaiResponse.status, headers: { 'Content-Type': 'application/json' } }
            );
        }

        // Modo nÃ£o-streaming: retornar JSON completo
        if (!streamMode) {
            const data = await openaiResponse.json();
            const content = data.choices?.[0]?.message?.content || '';
            return new Response(
                JSON.stringify({ content }),
                { status: 200, headers: { 'Content-Type': 'application/json' } }
            );
        }

        // Modo streaming: retransmitir SSE diretamente
        return new Response(openaiResponse.body, {
            status: 200,
            headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                Connection: 'keep-alive',
                'X-Accel-Buffering': 'no',
            },
        });
    } catch (error) {
        console.error('Chat handler error:', error);
        return new Response(
            JSON.stringify({
                error: error instanceof Error ? error.message : 'Erro interno do servidor',
            }),
            { status: 500, headers: { 'Content-Type': 'application/json' } }
        );
    }
}
